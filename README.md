My most recent mini project involves an imaginary Retail Corporation Inc that owns many service stations that sell fuel, the firm has billions 
in revenue and they would like to turn their cost centers into profit centers, fuel stations being one of them. 
Doing this in a repeatable and environmentally responsible manner requires a comprehensive program that goes beyond the scope of this project. Instead we focus on a subset of operations that may be ripe for pruning in the sense that more effective data pipelines can be built to reduce workload for operations managers, technicians and trim billable hours. An article concerning BP recently stumbled upon prompted this idea  (https://www.wired.com/wiredinsider/2019/12/bp-reimagining-fuel-stations-machine-learning-iot/)

For those short on time summary is at bottom and scripts in other files.

What the article says:

BP is looking to leverage a network of sensors and machine learning to perform preventative maintenance, it is also looking at using an automated voice response 
system on its technician support line in order to help with troubleshooting issues – this thus far has only been implemented on a small subset of locations.

Possible they may run into:
Couple of things came to my mind when thinking about how a poor rollout of a program like this could easily baloon in costs and complexity beyond what was planned and in some cases could misalign incentives.
To adress the above points we need to realize that an all-cloud solution can get probitively expensive once the processing and storage needs reach a certain level - instead a hybrid/tailored approach is likely the most sensible one.

One such approach could be a pipeline that is for exploratory purposes: 
POS transaction data  -> ftp -> aws storage gateway -> amazon s3 -> amazon athena -> quicksight/tableau

Another one we could use for specific alarm monitoring, which is what our project does on a small scale:
machine data logs -> private web server or ftp -> ec2 instance -> kinesis firehose -> aws s3 -> lambda trigger -> sns message

Both of the above pipelines make it clear who the data is tailored for and how it is to be used, one is exploratory to find insights or track performance while the other provides a quick response mechanism for certain evens.


What the project does:

The project uses a small set of machine data that is already generated by a modern gas station, meaning that there is relatively little capex needed as opposed
to updating all equipment across the board with bells and whistles.
ACME Inc is a firm that owns many service stations across the country. They decide to more proactively make use of the generated information to drive an asses 
parts of operational efficiency. In one of their data pipelines they have begun using Kinesis Firehose Streams to capture ATG(automatic tank gauge), Alarm and 
POS data from several sources of machine data, aggregating them on an EC2 instance which then publishes to a data lake housed in S3 . Once this is done there 
is a Lambda trigger that is hit and uses AWS’s python boto3 SDK to perform some scans that aggregate key metrics they are looking for at the time and if a certain 
condition is met an SNS message is sent to management (in this case my email).

Rule/Specification: 

we need to be alerted by email when a machine has its default alarm turned off , we will need the site and the time that this was 
identified. The machines gather data continuously but send it to our servers each 5 minutes as we deem this to be reasonable. Since this is a high priority event, 
the alerts will continue until the manager(s) call the site to find out what is happening and get them to turn the default alarm back on. Otherwise we could be 
having a leak or equipment fault and not know until a customer complaints or there is an oil leak.



What was actually done:

1) First thing to do is create an S3 Bucket, we will use the fake bucket name of ‘s3fuelbucket’ here. Next thing we will do here is 
to create the Kinesis Firehose stream with the fake stream name ‘fuelalarmsstream’ and the following configuration parameters:

    •	use direct put

    •	create/use IAM role that has permission to access s3

    •	we will be using AWS kinesis agent , the configuration will be on our EC2 instance

    •	set buffer conditions to say 60 or 300 seconds ,or to a specific file size you would like to split the streams in. 

    •	enable CloudWatch for error logging for kinesis

2) Next up is creating an EC2 instance

    •	Create EC2 instance on a t2.micro instance type, select a free tier Linux distribution, other default settings are fine. Get information of access/key pem file and save it.

    •	Download putty, then go onto Puttygen and use .pem file we just saved to help us login .Use our hostname/address of EC2 instance this will be used in putty

    •	Open putty, insert hostname, get into the SSH-AUTH area and select your ppk file that will serve as credentials. Save this setting so that you can login faster in the future

  Once EC2 instance is created and login info is saved we can login.

  VM machine will show up asking for user, type EC2-user as it is currently the default user id, install the below list:

    •	Sudo yum install –y aws-kinesis-agent

    •	Sudo amazon-linux-extras install python3.8

    •	Sudo easy_install pip

    •	Sudo pip install pandas

  Now we will create directory where we will save our fake machine log files:

    Sudo mkdir /var/log/fuelstream5min

  Set up and configure the kinesis agent, we use nano as an editor:

    Cd /etc/aws-kinesis

    Sudo nano agent.json

  Within the editor we enter our stream info and the destination folder, make sure that we do not enter secret/access credentials in this file as this is a security risk.

  Instead we can attach/modify the IAM role that is attributed to our EC2 instance and add the necessary permission to do some admin tasks on EC2

    (see ksconfig file for configuration of ks agent)


    We can then start our kinesis agent and configure it to start automatically

    Sudo service aws-kinesis-agent start

    Sudo chkconfig aws-kinesis-agent on

3)Now we are ready to get our fake machine data, there are many ways to do it, you may use wget if you have a server with link to the file, 
for ease of use here i just used Filezilla to create a connection with my EC2 instance though SFTP - credentials

From my ppk are used. This way i can just drag and drop the python file that will generate the fake machine data.

Once the file is in our home directory, change the permissions on it so that we can execute chmod a+x filename.py

    Chmod a+x FuelStream1Demo.py

4) Go onto lambda console and create a lambda function with required access to s3 and ability to communicate to SNS, allow for CloudWatch so 
that we may monitor our errors (also need to go to CloudWatch to create 'subscription' for this error logs)

  Type the script in boto3 python that essentially does the below:

  Call S3 client, Take JSON information from bucket 's3fuelbucket' that contains file that was just ingested, since we do not know the name we give 
  it the variable  of ‘filename’ that is an alias to the location of the specific file. Use getobject method and decode that so you may put in variable. 
  We then use S3 and if our query returns certain set of info we will append it to a list then we will use the SNS to notify manager, with a list of the
  locations of interest.

(see lambda file for code)

5) Connect the triggers

6) Create SNS subscription here give email

7) Run the python script

    Sudo python FuelStream1Demo.py





The above is an example of how in practice cloud infrastructure and computing can help managers and practitioners make better use of information at their disposal
instead of bombarding themselves with things they may not even use.
Obviously there are likely better ways to scan an incoming S3 file in Lambda-like trigger than using an S3 Select, but many of the methods I have seen thus far 
are above my pay grade and geared towards other use cases.

Summary:

Retailers that have modern infrastructure (alarm system, automatic tank gauges etc.) are able to leverage cloud services and their domain expertise to make use of 
machine data, helping drive operational efficiency gains. In this mini project we covered a situation where relevant and practical data is used to enhance the decision 
making ability of employees; we created a simplified data pipeline where  fake machine data log is generated by a python script, it is saved to a directory in an EC2 
instance from which Kinesis Firehose will pull data and forward it to an S3 bucket, once this is done a lambda function is triggered that performs a scan on the 
recently saved log file and if a certain condition is met (a certain alarm is set to OFF when it shouldn’t  etc) then it uses the SNS service to notify department 
head immediately. The tool is meant to serve as a quick response mechanism for events that are deemed to be time sensitive and actionable within the context of the 
business – instead of bombarding employees with data that they will now need to comb through to find something useful.

Where this comes in handy is that now you have a documented workflow that makes it clear what happened and what needs to be done, you are also managing risk by 
prioritizing worst case scenarios and the response time you would have to them, beyond that the modularity allows for the partitioning of information based on 
department – you may have a data science or analyst team focus on the transactional POS data that comes in batches (instead of stream) and they attempt to find 
the most profitable mix of products at the concession stand while the operations team focuses on the live machine information that may focus on signs of theft, 
alarms/leaks or even malfunctioning equipment. 

One specific application of machine learning that can tangibly be used is estimating the useful life/lifecycle of a dispenser of a specific model, this may allow 
for planning further ahead in time when purchasing hardware, if purchases are made in bulk then there are likely cost savings to be had.
This is a work in progress, so in the future I will hopefully add more detail, polish and completeness so that it may read like a real report.
I hope you enjoyed this tidbit 

